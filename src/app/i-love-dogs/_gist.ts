export const title =
  '# I love dogs\n\n> <span style="font-size:12px !important">by [Francisco Miranda](https://x.com/panchito), __Jan 2026__</span>\n\n> **TL;DR — I built a version of the GitHub pull request experience that feels instantly responsive, dramatically improves navigation and diff rendering for large PRs — so reviewing code stays focused on understanding, not waiting. You can check the [repo](https://github.com/cuquo/fast-github-poc) or try it below.**\n\n\n';
export const gist =
  'I really do, and I owe them more than I can explain. They’re part of my family — and in many ways, they’re the reason I have one. I met my wife and her dog while walking mine in the park.\n\nWhenever I get stuck on a problem, my dog wagging her tail and asking to go outside forces a pause. And that pause often gives me a new perspective.\n\nI try to be a responsible owner. I don’t anthropomorphize my dogs, and I’ve spent time training them. If my dog ever bites someone, that’s on me — I’m accountable.\n\nAnd everything I just said applies to AI, too.\n\nFollowing a classic three-act structure — adapted to software development — we can roughly split the workflow into three parts.\n\nThe **introduction** is where work is conceived. Tasks are created, issues are triaged, and ideas take shape. This is where companies like [Linear](https://linear.app) already outperform GitHub: AI helps categorize, prioritize, and even automate large parts of this phase with very little friction.\n\nThe **central development** is rapidly shifting toward AI as well. Tools like [Cursor](https://cursor.com) are leading the way, but they’re not alone. Everyone is fighting for this space — whether it’s an agent running in your terminal or a fully AI-powered platform like [V0](https://v0.app).\n\nThe **conclusion** is the part that interests me the most — and it should interest GitHub too. This is where code gets reviewed, approved, and shipped. This part of the workflow should be deeply user-centered, giving developers the right tools to understand changes and move confidently, the way tools like [Graphite](https://graphite.com) do.\n\nAI will almost certainly dominate the introduction and much of the central development. But the conclusion feels different.  \nAI won’t rant on social media if an MCP call takes too long. Humans will complain if reviewing a pull request is slow.\n\n**GitHub has a unique opportunity.**\n\nIt’s the only platform that spans the entire development lifecycle — from planning and coding to reviewing and shipping. Like many companies, GitHub has felt pressure to integrate AI, often in the form of chatbots or RAG-based assistants.\n\nThat’s fine. But the real opportunity isn’t another chatbot.\n\nIt’s using AI in a way that quietly supports developers across the workflow — especially in the part where humans are most involved: reviewing pull requests.\n\nThat’s what this post focuses on, and it’s also what nerd-sniped me in the first place. The conclusion of the workflow — reviewing, understanding, and approving code — should be centered on the human experience. AI can help, but the baseline must be a fast, calm, and predictable review experience. Everything starts with speed.\n\n## Classic vs. New: two experiences, different trade-offs\n\nToday, GitHub offers two pull request experiences: the classic view and the new one. It’s clear the new experience is the future, and GitHub is actively iterating on it — but both come with limitations.\n\nFor small PRs, the new experience feels noticeably faster than the classic one.  \nFor larger PRs, things get more complicated.\n\nAt first glance, the most visible change is the sidebar — and that’s not a small detail. GitHub now groups files by folders and then sorts them alphabetically, mirroring how repositories themselves are browsed.\n\n<img width="1500" height="1160" alt="new sidebar" src="https://gist.github.com/user-attachments/assets/d1e409df-6054-4e9b-baa3-69599ab4bb50" />\n\n<img width="1500" height="1160" alt="old sidebar" src="https://gist.github.com/user-attachments/assets/4579e607-06ce-453b-a5db-85db56494e5a" />\n\nThe classic view feels cluttered and noisy in comparison. The new one feels natural.\n\nThis small change has a big impact: reviewers can focus on a specific folder or package first — especially in monorepos — instead of being forced to start with something arbitrary like a modified `package.json`.\n\nThat’s a huge win for users.\n\nIt’s also bad news for anyone trying to replicate it, including this POC — and likely for GitHub’s own API consumers — since the API still returns files in the old order. Extra logic was required here to match the new behavior.\n\nThe icons are also simpler and clearer.\n\n<p>\n<img width="552" height="202" alt="old file" src="https://gist.github.com/user-attachments/assets/892ab1db-dd65-4e85-8f97-b50cf455dc46" />\n</p>\nInstead of separate glyphs for file type and change type, both are merged into a single symbol. Less noise, more signal.\n<p>\n<img width="552" height="202" alt="new file" src="https://gist.github.com/user-attachments/assets/ace9b7ab-ad26-4594-b947-3fbfdfccfe12" />\n</p>\n\n## Sidebar performance\n\nPerformance-wise, the new sidebar is buttery smooth. GitHub uses `content-visibility` effectively to limit painting work.\n\nThe classic experience, by contrast, now feels visually saturated — at least after spending time with the new one. In large PRs, you can even perceive small delays when hovering over entries.\n\n![hover gif](https://gist.github.com/user-attachments/assets/92769c10-0096-45d3-821e-14059efe8ee3)\n\n## Diff entry readability\n\nDiff entries also received subtle but meaningful improvements.\n\nThe five blocks representing changes were moved to the right, and additions and deletions are now easier to read. In the classic view, something like this felt like reading a binary clock.\n\n<img width="70" height="28" alt="image" src="https://gist.github.com/user-attachments/assets/e1ed796b-ef27-4b91-a54a-b051e251cc25" />\n\nThe new experience makes this explicit and readable.\n\n<img width="101" height="38" alt="Screenshot 2025-12-23 at 7 01 44 p m" src="https://gist.github.com/user-attachments/assets/ed44bd99-57d2-44ab-b422-614f745cb5d7" />\n\nFile names are also clearer, and long paths now truncate from the left instead of the right — preserving the most meaningful part of the name.\n\n<img width="379" height="149" alt="image" src="https://gist.github.com/user-attachments/assets/494dcfe2-b6c2-432a-ae35-caf0f0853488" />\n\n## Where the new experience breaks down\n\nDespite these improvements, the new experience has important limitations.\n\nThe classic view technically doesn’t limit how many files can be shown — but it only renders the first 100 diff entries, collapsing the rest into placeholders.\n\n<img width="1064" height="205" alt="Screenshot 2025-12-23 at 7 18 24 p m" src="https://gist.github.com/user-attachments/assets/b1383e80-aa4e-47f3-bd50-3b8af99d16d5" />\n\nThe new experience uses a different heuristic. Depending on the number of changed lines, it may render only a single file — as in [this PR](https://github.com/facebook/react/pull/34269/changes).\n\nFor very large PRs, GitHub eventually recommends switching back to the classic view.\n\nThe new experience is clearly optimized for small PRs. When it works, it eagerly renders most diff entries without lazy loading.\n\nBut for medium and large PRs, it switches to incremental loading — and this is where things fall apart.\n\n## The real problem: sequential loading\n\nWhen a PR crosses GitHub’s internal threshold for “medium” or “large,” the experience changes:\n\n- Only 4 diff entries are rendered initially\n- Skeletons are assigned a fixed height (in multiples of 50)\n- Entries are then loaded **sequentially**, in chunks of 4\n\nCrucially, entries are **not loaded based on what the user is looking at**, but based on what comes next in order.\n\nA PR like [this one](https://github.com/facebook/react/pull/33146/changes) loads **42 entries in a waterfall**. If you scroll quickly to the middle, you may need to wait several seconds before the file you want even exists in the DOM.\n\nEverything is painted on the client, so while you’re waiting for chunks to load, the main thread is also busy parsing and rendering.\n\n## A possible direction\n\nIf neither streaming nor server-side rendering is currently an option, React 19 opens new doors.\n\nRSC payloads could ship serialized content instead of forcing the client to parse and paint everything. React 19 also introduced [Activity](https://react.dev/reference/react/Activity), allowing parts of the UI to be marked as low priority to keep the main thread responsive.\n\nThere’s a great deep dive on this [here](https://calendar.perfplanet.com/2025/react-19-2-further-advances-inp-optimization).\n\nCombined with smarter intersection-based loading, this could significantly improve the experience of large PRs without sacrificing responsiveness.\n\n## The POC\n\nMy approach followed the same principles as the first part: keep client components minimal, render as much as possible on the server, and let the browser do less work upfront.\n\nThe goal was simple: keep the DOM small, avoid unnecessary client-side painting, and rely on techniques like `content-visibility` so the UI only does work when it actually needs to.\n\nTo explore this space, I built a [small playground](https://github.com/cuquo/fast-github-poc/blob/main/src/app/size/page.tsx) that mimics the structure of a pull request layout. This allowed me to experiment with different rendering strategies, DOM sizes, and layout constraints before committing to a full implementation.\n\nIn parallel, I needed real-world stress cases. I asked ChatGPT to help me put together a [simple script](https://github.com/cuquo/fast-github-poc/blob/main/bigpr.ts) to collect some of the [largest pull requests](https://github.com/cuquo/fast-github-poc/blob/main/big-prs-results.json) from the React repository, and used those PRs as test inputs throughout the process.\n\nThose PRs became the baseline for validating ideas, breaking assumptions, and measuring whether changes actually improved the experience or just felt clever.\n\n## Findings\n\nDOM size matters more than it looks.\n\n- Every extra DOM node has a cost. Even elements that aren’t painted still participate in layout and style calculations, and that cost becomes very noticeable during window resizes.\n- Rendering a very large number of elements upfront directly impacts First Contentful Paint. In extreme cases, the browser can appear frozen while the DOM is being constructed.\n- Gradually inserting elements into the DOM wasn’t an optimization — it was required to keep the experience usable.\n- In practice, anything below ~300 files kept the interface consistently smooth, even during resizes.\n- For larger PRs, `content-visibility` is non-negotiable. Without it, performance degrades fast. With it, scrolling stays fluid even when the DOM grows significantly.\n- Exact height measurement for diff entries isn’t realistic in pull requests. Blob views are predictable, but diffs break the assumption: long lines wrap, context changes, and a single line can become two or three.\n- Perfect measurement isn’t necessary. Approximate heights work surprisingly well. I validated this by navigating rapidly between files using the sidebar: layout shifts didn’t push the selected file out of view, and the experience remained stable.\n\n## Sidebar\n\nThe new GitHub experience sorts files by folders first and then root files. To match that behavior, I had to load *all* sidebar pages via GraphQL before the first paint. That decision immediately cascaded into the diff entries logic, but it was the right starting point if I wanted to faithfully adopt the new visual model.\n\nImproving the sidebar was surprisingly constrained. GitHub already uses `content-visibility` and `contain-intrinsic-size`, and those two account for most of the performance gains. That meant the biggest remaining lever was DOM size.\n\nI focused on reducing the number of elements while preserving the same visual output. The sidebar went from 11 elements per entry down to 8. That doesn’t sound dramatic, but on a pull request with 3,000 files, that reduction alone is roughly equivalent to the initial DOM size of the New York Times homepage.\n\nI did add one extra element: an `<input>`. That technically increases the DOM count for folders by one, but it allowed me to handle folder collapsing entirely in CSS instead of JavaScript. The trade-off was worth it — simpler logic and fewer runtime costs.\n\nThere’s an interesting CSS property called `overflow-anchor`, supported in all modern browsers except Safari, that helps prevent scroll position jumps by adjusting the scroll anchor automatically. It’s enabled by default (`auto`).\n\n<img width="888" height="483" alt="image" src="https://gist.github.com/user-attachments/assets/f6d3d499-1f28-4278-bad9-3eacc5e4cb64" />\n\nBecause every sidebar entry has a fixed height, disabling `overflow-anchor` in supported browsers gave me small but measurable gains, helping the UI consistently hit the maximum frame rate during fast scrolling.\n\nAnother subtle improvement came from paint optimization. Transparency isn’t a problem in most interfaces, but in a UI that scales to thousands of rows, alpha blending adds up. By removing unnecessary transparency and avoiding alpha layers, painting became slightly faster and more predictable.\n\nTo keep the comparison fair, I avoided adding new features. The only exception was scroll markers for pull requests with 100 files or fewer. I experimented with enabling them for larger PRs, but the scroll experience degraded quickly, so I limited their use to smaller cases.\n\n![scroll-markers](https://gist.github.com/user-attachments/assets/4cc1903e-626b-4b1f-a121-89aa74b74b09)\n\nAfter finishing the work on diff entries, I came back to the sidebar with fresh context.\n\nAt that point, it no longer made sense to keep using GraphQL there. I switched the sidebar data source to REST instead, allowing both the sidebar and diff entries to reuse the same endpoint and benefit from the same caching and deduplication logic.\n\nThat change simplified the data flow, reduced duplicate fetches, and made the overall system easier to reason about — especially under load.\n\n## Diff Entries\n\nThis was the hardest part of the project — and it kept getting harder the deeper I went. Diff entries look deceptively simple, but there’s a lot happening under the hood.\n\nMy initial plan was straightforward:\n\n- Fetch patches\n- Parse them\n- Calculate an approximate height on the server (for `contain-intrinsic-size`)\n- Use `content-visibility` to limit rendering\n- Add syntax highlighting\n\n## Fetching and first rendering\n\nThis is where the GraphQL road ends. Patches are only available via REST, the rate limits are tighter than GraphQL’s, and the new file-sorting logic made things more complex.\n\nI used Octokit to fetch patches. That part was mostly painless, except for one important tweak: disabling throttling. The default behavior interfered with streaming and introduced unnecessary delays.\n\nOnce fetching was in place, I moved on to parsing patches into something renderable. ChatGPT handled that surprisingly well — within a couple of hours I had diff entries rendering correctly, without obvious visual issues.\n\nCalculating approximate heights on the server and adding `content-visibility` took minutes.\n\nAt that point, I had a working POC in a single day — everything except syntax highlighting. I tested it on small PRs first, then larger ones, and finally on very large PRs.\n\nThat’s when things started to fall apart. Performance wasn’t great — in some cases it was worse than GitHub’s. I also saw random crashes and freezes that I initially dismissed as local issues. But after reproducing the same behavior with Next.js, Turbopack, and Rspack, it became clear this wasn’t just a local issue.\n\nMy first fetch strategy was split in two steps:\n\n1. Load the first 100 patches, split into root and non-root files\n2. Fetch the rest, merge everything, then sort\n\nIt worked functionally — and for small PRs, it behaved just fine. Fetching and painting a few hundred files in one go stayed within reasonable limits, and the UI remained responsive.\n\nThe issue only became real as PRs grew larger. The same approach turned a single render pass into thousands of DOM nodes competing for layout, style, and memory at once.\n\nFetching wasn’t the real issue — it was the amount of global, sequential work that had to complete before any meaningful UI could appear.\n\n<img\n  width="1277"\n  height="192"\n  alt="Initial fetch strategy: global merge and reorder before processing and paint"\n  src="https://gist.github.com/user-attachments/assets/8ee966e6-9fd7-437a-b17d-bacc99ddc0c1"\n/>\n\n## Syntax highlighting: the first wrong turn\n\nNext on the list was syntax highlighting. I tried the same approach I had used for blobs: calling GitHub’s markup REST endpoint.\n\nThat worked — briefly. On large PRs, I hit API rate limits almost immediately.\n\nSo I tried a different approach: render everything on the server *without* syntax highlighting, then apply highlighting on the client using Intersection Observers.\n\nIt was usable, but not good. Scrolling felt uneven, and rate limits were still a problem.\n\nThat’s when I switched to server-side highlighting with [Shiki](https://shiki.style). The tradeoff was clear: unlike GitHub’s API, Shiki can’t infer language automatically — I had to add logic to determine it explicitly. Fortunately, that was manageable.\n\nAround this time, I changed the fetch pattern again. I introduced multiple queries and additional Suspense boundaries to avoid blocking the main thread while painting.\n\nThe goal was to give the browser some “air” to render.\n\n<img\n  width="1553"\n  height="301"\n  alt="Second fetch strategy: incremental fetch and stitching with additional Suspense boundaries"\n  src="https://gist.github.com/user-attachments/assets/6750606e-1677-405b-84d3-e63df554db25"\n/>\n\nOn paper, it made sense.\n\nIn practice, it was worse.\n\n## Suspense waterfalls and streaming pain\n\nThe problem wasn’t just fetching anymore. React Suspense blocks not only on data fetching, but on *any* async work inside a boundary — including parsing, transforming, and rendering Shiki output.\n\nIn practice, it became a massive waterfall\n\n<img width="1319" height="360" alt="image" src="https://gist.github.com/user-attachments/assets/7555359f-f035-4098-8705-e5ca488506b4" />\n\nPerformance per frame improved, but total streaming time exploded. Large PRs could take a minute or more before becoming usable.\n\nAt first, I suspected Octokit throttling or cache issues. After measuring cache hit rates both locally and on Vercel, it was clear that wasn’t the culprit.\n\nSo I changed the strategy again.\n\n## Streaming cadence and adaptive chunking\n\nCached requests were effectively free — milliseconds locally and in production. So I returned to a two-phase approach:\n\n- Fetch the first set of files eagerly\n- Fetch the remaining files in a second pass\n\nThis time, instead of fixed chunks, I split diff entries into chunks of ~30 files, each wrapped in its own Suspense boundary, with small delays between them.\n\nScrolling was always smooth (thanks to `content-visibility`), but the overall experience still didn’t feel *snappy*.\n\nThat’s when I added performance markers and brought in Gemini for a second opinion. I profiled different PR sizes, varied chunk sizes and delays, and fed those traces back into Gemini to compare outcomes quickly.\n\nEventually, instead of hardcoding chunk sizes, I made them dynamic — based on the approximate number of lines derived from patch metadata. That allowed chunk sizes to adapt to PR size and complexity.\n\nThe experience improved noticeably.\n\nBut the bugs kept coming.\n\n## Diff algorithms: when memory becomes the bottleneck\n\nThe biggest remaining issue turned out to be the diff algorithm itself.\n\nTo render side-by-side diffs, I needed to transform patches into “old lines” and “new lines”. My initial LCS implementation — generated by ChatGPT 5.2 — used classic dynamic programming.\n\nIt worked beautifully on small files.\n\nIt did *not* work well at scale.\n\nWith many async tasks running, large files, and a serverless environment, the algorithm consumed too much memory and pushed the garbage collector hard. ChatGPT had already warned me this might happen and suggested Myers’ algorithm as a more memory-efficient alternative.\n\nSo I asked Gemini.\n\nGemini suggested a clever optimization: use a single `Int32Array` instead of nested arrays to reduce memory pressure. I searched for an existing JavaScript diff library using Myers with these constraints and found nothing suitable.\n\nSo I asked Gemini to write one.\n\nIt generated the code fast — very fast — and labeled it “production ready”. I wasn’t convinced, so I asked ChatGPT to review it.\n\n> “LGTM on approach, NACK on implementation. Needs rewrite with array-based Myers + strict invariants.”\n\nWith that feedback, Gemini rewrote the implementation. I kept ChatGPT as a reviewer and Gemini as the primary coder until both models agreed the code was solid.\n\nWhat followed was about half an hour of copy-pasting between ChatGPT and Gemini — a ridiculous sycophancy battle where both models kept praising each other — until they declared the final version *“beyond gold master, this is dynamite.”*\n\nRidiculous phrasing aside, the result was a Myers diff implementation with predictable memory usage and no GC spikes.\n\n## Benchmarks\n\nAt this point, I stopped trusting intuition and started measuring.\n\nBoth models also collaborated on the benchmarks themselves.  \nThe tests were reviewed and adjusted multiple times to ensure we were **comparing apples to apples**.\n\n<p>\n<img width="100" height="100" alt="20k lines performance rev2" hspace="5" src="https://gist.github.com/user-attachments/assets/6879efd8-f6c6-4361-a05f-dfea8d795406" />\n<img width="100" height="100" alt="20k lines memory rev2" hspace="5" src="https://gist.github.com/user-attachments/assets/89d277d3-b4b7-496a-9006-056cdbb04fa6" />\n<img width="100" height="100" alt="jquery performance rev2" hspace="5" src="https://gist.github.com/user-attachments/assets/0e5b76ff-b00e-46a7-b22a-05a28ef10100" />\n<img width="100" height="100" alt="jquery memory rev2" hspace="5" src="https://gist.github.com/user-attachments/assets/5d2e12ea-0b84-4a83-b226-c2adebbbc720" />\n</p>\n\nThe custom implementation (`custom-diff`) was benchmarked against:\n\n- [`fast-diff`](https://www.npmjs.com/package/fast-diff)\n- [`jsdiff`](https://www.npmjs.com/package/diff) (`diffLines`)\n\nIn all cases I measured **performance** and **memory usage** under two different scenarios:\n\n1. **20k lines with random changes** (worst-case stress test)\n2. **jQuery source code with random changes** (real-world diff)\n\nAll benchmarks were run using `hyperfine` with warmup runs and repeated measurements.[^1]\n\n[^1]:*Numbers are rounded from the hyperfine mean.*\n\n---\n\n### Test 1: 20k lines (worst case)\n\n#### Performance\n\n| Algorithm       | Mean time |\n|-----------------|-----------|\n| **custom-diff** | **~29 ms** |\n| jsdiff          | ~32 ms |\n| fast-diff       | ~35 ms |\n\nEven in this synthetic worst-case scenario, the custom implementation performs slightly better:\n\n- ~**1.2× faster than fast-diff**\n- ~**1.1× faster than jsdiff**\n\nPerformance differences here are modest, as all three algorithms operate in similar theoretical bounds. Memory behavior is where things diverge sharply.\n\n#### Memory (retained outputs)\n\n| Algorithm | Total retained heap (20 runs) | Avg retained per run |\n|----------|-------------------------------|----------------------|\n| **custom-diff** | **~0 MB** | **~0 MB** |\n| fast-diff (line tokens) | ~17 MB | ~0.87 MB |\n| fast-diff (char-level) | ~61 MB | ~3.08 MB |\n| jsdiff | **~110 MB** | **~5.5 MB** |\n\nThe difference is stark:\n\n- **custom-diff** retains virtually no memory across runs\n- **fast-diff** retains moderate memory depending on usage\n- **jsdiff** allocates and retains a large amount of heap memory\n\nIn serverless or concurrent environments, this kind of retained memory quickly translates into GC pressure and unpredictable latency.\n\n---\n\n### Test 2: jQuery (real-world diff)\n\n#### Performance\n\n| Algorithm       | Mean time |\n|-----------------|-----------|\n| **custom-diff** | **~44 ms** |\n| fast-diff       | ~300 ms |\n| jsdiff          | ~1.0 s |\n\nOn a realistic codebase, the gap widens dramatically:\n\n- **custom-diff is ~6.7× faster than fast-diff**\n- **~24× faster than jsdiff**\n\nHere, object-heavy representations and extra preprocessing costs dominate runtime for the existing libraries.\n\n#### Memory\n\n| Algorithm | Peak heap |\n|----------|-----------|\n| **custom-diff** | **~8 MB** |\n| fast-diff | ~40–47 MB |\n| jsdiff | **~126 MB** |\n\nAgain, the custom implementation keeps memory flat and predictable, while `jsdiff` in particular drives heap usage high enough to cause long GC cycles.\n\n## Why this matters\n\nWhile raw performance improvements on synthetic tests are incremental, **memory behavior is the real differentiator**.\n\nThe custom Myers implementation uses flat `Int32Array` structures and avoids object-heavy allocations, resulting in:\n\n- Predictable memory usage\n- No retained heap growth\n- No GC spikes under load\n\nIn realistic scenarios, this translates into **significantly faster diffs** and **far lower memory pressure**.\n\nFor serverless, streaming, or highly concurrent systems, this tradeoff matters far more than theoretical algorithmic complexity — and this is where the custom implementation clearly wins.\n\n## Caching: necessary, painful, unavoidable\n\nThe next class of bugs was caching.\n\nCaching is deeply underrated — and incredibly hard to get right at scale.\n\nNext.js [Cache Components](https://nextjs.org/docs/app/getting-started/cache-components) aren’t perfect, but they’re clearly moving in the right direction. They give developers fine-grained control over what gets cached, for how long, and whether we’re caching fetch responses or the full RSC payload of expensive computations.\n\nThat said, cache keys are derived from input parameters. In practice, this meant I couldn’t pass raw diff content or Shiki output directly without exploding the cache key space.\n\nInstead, I had to cache by reference: page index, file index, and identifiers tied back to already-cached queries. In Redis, this would have been trivial — keying directly by file SHA — but that wasn’t an option here.\n\nOn top of that, aggressive caching combined with fully async execution introduced another problem: connection limits. Locally, the cache lives in an in-memory LRU. In production, it’s backed by Vercel’s cache or Redis, both with hard limits on concurrent connections.\n\nTo avoid thundering herds, I introduced batching and in-flight deduplication across several helpers — while still keeping everything async.\n\n## The final bug: Chrome\n\nThe last bug wasn’t in my code.\n\nI added `scroll-target-group` to the sidebar to improve navigation. Unfortunately, Chrome continuously measures scroll targets while content is still streaming.\n\nBecause diff entries were still being added over time, Chrome entered an infinite measurement loop and eventually crashed.\n\n<p align="center">\n<img width="1500" height="820" alt="crash" src="https://gist.github.com/user-attachments/assets/1fe78d51-5ab6-4eba-9c61-9d92d8d44698" />\n</p>\n\nThe fix was pragmatic: a small client component that waits until the last item of the final chunk loads, then enables `scroll-target-group` via a class toggle.\n\n## Magic\n\nNot every performance win comes from optimizing algorithms.\n\nSometimes the right move is to stop optimizing the process and start using small, almost silly tricks. Like magic: once you know how it works, it sounds obvious — but the first time you see it, it genuinely surprises you.\n\nThe repository view has one of those tricks. Thanks to the GitHub API, we know a file’s size ahead of time. That lets me fully prefetch small files (≤ 20 KB), while larger files render skeletons first. Navigation feels instant, bandwidth is preserved, and the browser avoids loading content that could otherwise push it toward memory pressure.\n\n![prefetch blob](https://gist.github.com/user-attachments/assets/a9a2de4d-3e7d-43ca-b536-d9c275ed7bdb)\n\nThe same idea applies to pull requests.\n\nIf a PR has [`DIFFS_PER_PAGE`](https://github.com/cuquo/fast-github-poc/blob/main/src/constants/options.ts#L8) files or fewer, I can fetch that number up front and fully preload the PR. Small PRs open instantly, without special cases or loading states.\n\n![prefetch pr](https://gist.github.com/user-attachments/assets/462d5423-9292-487b-ba22-1715999428aa)\n\nSyntax highlighting needed a similar trick.\n\nShiki performs well locally, but in production it’s more expensive. The solution was [`INITIAL_SYNTAX_HIGHLIGHT_DIFF`](https://github.com/cuquo/fast-github-poc/blob/main/src/constants/options.ts#L11) cache a small number of highlighted diffs as HTML directly in the initial payload.\n\nNo matter how large the PR is, the first 20 entries (`DIFFS_PER_PAGE`) load immediately, and 5 of those are highlighted synchronously on page load. As the user scrolls, the remaining batches continue processing in the background — without ever blocking interaction or making scrolling feel heavy.\n\n## Conclusion\n\nI really enjoyed working on this project.\n\nThese are the kinds of challenges that originally pulled me into programming — slowing down, questioning assumptions, and caring about how things *feel* to use.\n\nAfter a long time away from that mindset, this reminded me that curiosity doesn’t disappear. You just have to give it room to come back. Turns out, old dogs can still learn new tricks.\n\nReviewing code is a human activity. Performance isn’t just about speed — it’s about calm, focus, and staying in flow while trying to understand someone else’s work.\n\nIf any of this resonates with you, try it out, explore the repo, and take whatever is useful. I’ll be more than happy if it sparks ideas, conversations, or better implementations.\n';
